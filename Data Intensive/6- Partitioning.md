وقتی اندازه دیتا بزرگ هست، قاعدتا مدت زمان اجرای کوئری ها هم زیاد هست. با کمک پارتیشنینگ دیتا رو به پارتیشن های کوچک تر تقسیم میکنیم.

به این ترتیب با کمک پارتیشنینگ query throughput بهبود پیدا میکند.

تو پارتیشن بندی دیتا، هر رکورد یا دایومنت دقیقا در یکی و فقط یکی از پارتیشن ها قرار خواهد گرفت. هر پارتیشن انگار برای خودش یه دیتابیس مستقل هست. امکان عملیات موازی در پارتیشن های متفاوت وجود دارد.

شیوه های مختلف پارتیشنگ رو در این فصل خواهیم دید.

یک دلیل اصلی برای پارتیشنینگ بالا بردن scalability هست. هر پارتیشن میتونه روی یک نود مستقل قرار بگیره. نود مستقل یعنی همون معماری shared-nothing که در صفحه ۱۴۶ بهش اشاره شد.
وقتی یه dataset بزرگ بین نودهای مستقل پخش بشه اینطوری لود کوئری ها هم روی پردازنده های مختلف پخش خواهد شد و در نتیجه query throughput بهبود پیدا میکنه.

برای اولین بار ۴۴ سال پیش، این دو دیتابیس برای پارتیشنینگ پیش قدم شدند.

پارتیشنینگ حتی در مورد دیتابیس هی noSQL که کاربرد analytics دارند هم به کار گرفته میشه.

# Partitioning and Replication

پارتیشنینگ معمولا با رپلیکیشن همزمان استفاده میشه. از هر پارتیشن به تعداد replication factor روی نودهای متفاوت پخش میشن. اینجوری دیگه fault tolerant هم هستیم.

وقتی رپلیکیشن با پارتیشنینگ همزمان استفاده میشه و مدل رپلیکیشن مون از نوع لیدر-فالوور هست مشابه شکل ۶-۱ برای هر کدوم از پارتیشن ها یکی از نودها نقش لیدر و بقیه نقش فالوور رو دارند. اینطوری هر نود برای برخی از پارتیشن ها نقش لیدر رو ایفا میکنه و برای بقیه پارتیشن ها نقش فالوور رو داره.

متدی که برای رپلیکیشن نتخاب میشه مستقل از متد پارتیشنینگ هست. در این فصل برای اینکه مساله رو ساده تر کنیم کلا رپلیکیشن رو درنظر نمیگیریم و فقط روی پارتیشنینگ تمرکز میکنیم.

# Partitioning of Key-Value Data
هدف پارتیشنینگ این هست که دیتا رو بین نودهای موجود طوری پخش کنه که بتونه scalability  و کوئری تروپوت رو متناسب با تعداد نودهای موجود افزایش بده.


اگر روش پارتیشن بندی fair نباشه ممکنه یک نود تبدیل به hot spot بشه و همه کوپری ها در نهایت به یک نود ختم بشن و بقیه نودها بیکار بمونن.

یک رویکرد ساده برای جلو گیری از hot spot اینه که رکوردها رو به صورت تصادفی بین نودها پخش کنیم. مشکل این روش اینه که موقع خواندن یک دیتا چون نمیدونیم اون دیتا دقیقا روی کدوم نود قرار داره مجبوریم به طور موازی روی همه نودها کوئری بزنیم.

یه رویکرد بهتر این میتونه باشه: 
فرض کنیم قالب داده هامون key-value هستن و ما دیتا ها رو از روی کلیدشون که منحصر به فرده پیدا میکنیم. شبیه دایره المعارف کاغذی قدیمی. وقتی تربیب هم بر اساس الفبا باشه خیلی راحت میشه دیتا رو پیدا کرد.

با این فرض چندین روش برای پارتیشن بندی داده های key-value طور وجود داره:

Partitioning by Key Range
Partitioning by Hash of Key
Skewed Workloads and Relieving Hot Spots

# Partitioning by Key Range
به هر نود یک رنجی از کلیدها رو اساین کنیم. همینکه بدونیم از چه کلیدی تا چه کلیدی روی کدوم پارتیشن هست موقع خوندن داده ها میتونیم به راحتی از روی کلید مورد جستجو بریم سراغ پارتیشن درست. و اینکه بدونی هر پارتیشن دقیقا روی چه نودی قرار داده به راحتی میرسیم به نود مقصد.

در این شکل داره نشون میده هر پارتیشن دقیقا از چه کلید تا چه کلیدی رو شامل میشه. البته در صفحه ۲۰۹ بیشتر راجع به توزیع کلیدها روی پارتیشن ها و انتخاب boundaries ها قراره صحبت بکنه.

داخل هر پارتیشن میشه کلیدها رو به صورت مرتب شده در قالب SSTable  و LSMTree نگهداری کرد. خوبی این روش اینه که اگر یه رنجی از دیتا رو میخواهیم بخوانیم با یه کوئری انجامش میدیم.

یکی از مشکلات روش key range partitioning اینه که بسته به الگوی دسترسی به داده ها ممکنه هات اسپات به وجود بیاد. مثلا اگر یه مجموعه سنسور داریم که دیتاشون خونده میشن و با کلید timestamp نگهداری میشن.در این سناریو نوشتن های روزانه دیتا باعث ایجاد hotspot میشه

برای اینکه این اتفاق نیفته باید یه دیتای دیگه ای به عنوان کلید انتخاب کنیم. یا اینکه نام سنسور رو به صورت prefix به اول timestamp اضافه کنیم. اینجوری اگر در یه لحظه از چندین سنسور دیتا خونده بشه عملیات نوشتن شون روی پارتیشن های مختلف پخش خواهد شد. 

# Partitioning by Hash of Key

برای حل مشکل هات اسپات میان از کلید هش میگیرن و از روی اون پارتیشن رو تشخیص میدن. اینجا لزومی نداره الگوریتم هشینگ قوی باشهو مثلا کاساندرا و مونگو دارن از MD5 استفاده میکنند.

 البته برخی از زبان های برنامه نویسی هش های پیاده سازی شون به درد پارتیشن بندی نمیخورن چون خیلی کانفلیکت پیش میاد توشون.
 با تیکنیک Consistent Hashing تو زیع دیتا روی پارتیشن های به صورت fair انجام میشه. در این روش boundaries پارتیشن ها از هم فاصله یکسانی خواهد داشت. 




وقتی داریم از کلیدها هش میگیریم یه قابلیت رو از دست میدیم. اونم جستجو به روش key-range هست. چون کلیدهای پشت سر هم دیگه لزوما در مکان های مجاور هم قرار نمیگیرند.

در مونگودیبی اگر مدل هشینگ رو فعال کنید، اگر range query بزنید کوئری شما روی تمام پارتیشن ها اجرا میشه

برخی دیتابیس ها هم اصلا range query رو پشتیبانی نمیکنند مثل 
Riak , Couchbase , or Voldemort

کاساندرا برای داشتن امکان range-query به همراه روش Partitioning by Hash of Key یه تکنیک جالبی استفاده میکنه که compound primary key نامیده میشه

به جای انتخاب یه column به عنوان کلید، از چندین کلید به صورت ترکیبی استفاده میکنه. با کمک column اول پارتیشن مقصد انتخاب میشه. از طریق مابقی columnها مکان دقیق آبجکت در SSTables مشخص میشه.

اینجا اگر اون column اول یه مقدار فیکس باشه میشه روی column های بعدی range query زد.

حالا این روش وقتی خیلی عالی عمل میکنه که رابطه  one-to-many داشته باشیم. 

مثلا یه کاربر تعدادی پست داره. در جدول پست ها ترکیب ستون (user_id, update_timestamp) حکم همون compound primary key  رو داره که با Hash(user_id) پارتیشن مشخص میشه و با update_timestamp روی همون پارتیشن به آبجکت اصلی میرسیم. که روی update_timestamp میشه range query هم زد.

تو این سناریو پست های کاربرهای مختلف روی پارتیشن های مختلف نگهداری میشن ولی همه پست های یک کاربر همگی روی یک پارتیشن هستند.


# Skewed Workloads and Relieving Hot Spots

چجوری از پدیده Skewed یا همان  Hot Spot Partition جلوگیری کنیم؟

هش گرفتن از کلید به این مساله کمک میکنه ولی نه به طور کامل. به خصوص اگر همه نوشتن ها یا خواندن ها روی یه کلید ثابت انجام بشه تمام workload روی یک پارتیشن متمرکز میشه.

مثلا فکر کنید در یک اپلیکیشن سوشال میدیا  یه سلبریتی که تعداد فالوورهای میلیونی داره یه اکشن انجام بده و باعث بشه یه طوفانی از write یا read به سمت یه پارتیشن هجوم بیاره

معمولا دیتاسیستم ها این skewed workload رو سعی نمیکنن که بهینه هندل کنند. پس باید اپلیکیشن خودش بیاد فکری به حال این workload بکنه.


یک راه حل پیشنهادی: اپلیکیشن بیاد یک عدد رندوم به ابتدا یا انتهای کلید ضمیمه بکنه. مثلا فقط دو رقم رندوم اضافه کنه و اینطوری عملیات نوشتن روی یک کلید به صورت رندوم بین ۱۰۰ تا پارتیشن پخش میشه. ولی؟؟ موقع خوندن هم باید خودش موضوع رو هندل کنه. داده های اون ۱۰۰ تا پارتیشن رو بخونه و ترکیب کنه.

این کار رو میشه فقط روی کلیدهایی انجام داد که نرخ نوشتن روی اونها زیاده. بقیه کلیدها رو لازم نیست اینکار رو باهاش بکنیم. پس باید کلیدهای از این جنس رو هم یادمون نگه داریم.




وقتی اندازه دیتا بزرگ هست، قاعدتا مدت زمان اجرای کوئری ها هم زیاد هست. با کمک پارتیشنینگ دیتا رو به پارتیشن های کوچک تر تقسیم میکنیم.

به این ترتیب با کمک پارتیشنینگ query throughput بهبود پیدا میکند.

تو پارتیشن بندی دیتا، هر رکورد یا دایومنت دقیقا در یکی و فقط یکی از پارتیشن ها قرار خواهد گرفت. هر پارتیشن انگار برای خودش یه دیتابیس مستقل هست. امکان عملیات موازی در پارتیشن های متفاوت وجود دارد.

شیوه های مختلف پارتیشنگ رو در این فصل خواهیم دید.

یک دلیل اصلی برای پارتیشنینگ بالا بردن scalability هست. هر پارتیشن میتونه روی یک نود مستقل قرار بگیره. نود مستقل یعنی همون معماری shared-nothing که در صفحه ۱۴۶ بهش اشاره شد.
وقتی یه dataset بزرگ بین نودهای مستقل پخش بشه اینطوری لود کوئری ها هم روی پردازنده های مختلف پخش خواهد شد و در نتیجه query throughput بهبود پیدا میکنه.

برای اولین بار ۴۴ سال پیش، این دو دیتابیس برای پارتیشنینگ پیش قدم شدند.

پارتیشنینگ حتی در مورد دیتابیس هی noSQL که کاربرد analytics دارند هم به کار گرفته میشه.

# Partitioning and Replication

پارتیشنینگ معمولا با رپلیکیشن همزمان استفاده میشه. از هر پارتیشن به تعداد replication factor روی نودهای متفاوت پخش میشن. اینجوری دیگه fault tolerant هم هستیم.

وقتی رپلیکیشن با پارتیشنینگ همزمان استفاده میشه و مدل رپلیکیشن مون از نوع لیدر-فالوور هست مشابه شکل ۶-۱ برای هر کدوم از پارتیشن ها یکی از نودها نقش لیدر و بقیه نقش فالوور رو دارند. اینطوری هر نود برای برخی از پارتیشن ها نقش لیدر رو ایفا میکنه و برای بقیه پارتیشن ها نقش فالوور رو داره.

متدی که برای رپلیکیشن نتخاب میشه مستقل از متد پارتیشنینگ هست. در این فصل برای اینکه مساله رو ساده تر کنیم کلا رپلیکیشن رو درنظر نمیگیریم و فقط روی پارتیشنینگ تمرکز میکنیم.

# Partitioning of Key-Value Data
هدف پارتیشنینگ این هست که دیتا رو بین نودهای موجود طوری پخش کنه که بتونه scalability  و کوئری تروپوت رو متناسب با تعداد نودهای موجود افزایش بده.


اگر روش پارتیشن بندی fair نباشه ممکنه یک نود تبدیل به hot spot بشه و همه کوپری ها در نهایت به یک نود ختم بشن و بقیه نودها بیکار بمونن.

یک رویکرد ساده برای جلو گیری از hot spot اینه که رکوردها رو به صورت تصادفی بین نودها پخش کنیم. مشکل این روش اینه که موقع خواندن یک دیتا چون نمیدونیم اون دیتا دقیقا روی کدوم نود قرار داره مجبوریم به طور موازی روی همه نودها کوئری بزنیم.

یه رویکرد بهتر این میتونه باشه: 
فرض کنیم قالب داده هامون key-value هستن و ما دیتا ها رو از روی کلیدشون که منحصر به فرده پیدا میکنیم. شبیه دایره المعارف کاغذی قدیمی. وقتی تربیب هم بر اساس الفبا باشه خیلی راحت میشه دیتا رو پیدا کرد.

با این فرض چندین روش برای پارتیشن بندی داده های key-value طور وجود داره:

Partitioning by Key Range
Partitioning by Hash of Key
Skewed Workloads and Relieving Hot Spots

# Partitioning by Key Range
به هر نود یک رنجی از کلیدها رو اساین کنیم. همینکه بدونیم از چه کلیدی تا چه کلیدی روی کدوم پارتیشن هست موقع خوندن داده ها میتونیم به راحتی از روی کلید مورد جستجو بریم سراغ پارتیشن درست. و اینکه بدونی هر پارتیشن دقیقا روی چه نودی قرار داده به راحتی میرسیم به نود مقصد.

در این شکل داره نشون میده هر پارتیشن دقیقا از چه کلید تا چه کلیدی رو شامل میشه. البته در صفحه ۲۰۹ بیشتر راجع به توزیع کلیدها روی پارتیشن ها و انتخاب boundaries ها قراره صحبت بکنه.

داخل هر پارتیشن میشه کلیدها رو به صورت مرتب شده در قالب SSTable  و LSMTree نگهداری کرد. خوبی این روش اینه که اگر یه رنجی از دیتا رو میخواهیم بخوانیم با یه کوئری انجامش میدیم.

یکی از مشکلات روش key range partitioning اینه که بسته به الگوی دسترسی به داده ها ممکنه هات اسپات به وجود بیاد. مثلا اگر یه مجموعه سنسور داریم که دیتاشون خونده میشن و با کلید timestamp نگهداری میشن.در این سناریو نوشتن های روزانه دیتا باعث ایجاد hotspot میشه

برای اینکه این اتفاق نیفته باید یه دیتای دیگه ای به عنوان کلید انتخاب کنیم. یا اینکه نام سنسور رو به صورت prefix به اول timestamp اضافه کنیم. اینجوری اگر در یه لحظه از چندین سنسور دیتا خونده بشه عملیات نوشتن شون روی پارتیشن های مختلف پخش خواهد شد. 

# Partitioning by Hash of Key

برای حل مشکل هات اسپات میان از کلید هش میگیرن و از روی اون پارتیشن رو تشخیص میدن. اینجا لزومی نداره الگوریتم هشینگ قوی باشهو مثلا کاساندرا و مونگو دارن از MD5 استفاده میکنند.

 البته برخی از زبان های برنامه نویسی هش های پیاده سازی شون به درد پارتیشن بندی نمیخورن چون خیلی کانفلیکت پیش میاد توشون.
 با تیکنیک Consistent Hashing تو زیع دیتا روی پارتیشن های به صورت fair انجام میشه. در این روش boundaries پارتیشن ها از هم فاصله یکسانی خواهد داشت. 




وقتی داریم از کلیدها هش میگیریم یه قابلیت رو از دست میدیم. اونم جستجو به روش key-range هست. چون کلیدهای پشت سر هم دیگه لزوما در مکان های مجاور هم قرار نمیگیرند.

در مونگودیبی اگر مدل هشینگ رو فعال کنید، اگر range query بزنید کوئری شما روی تمام پارتیشن ها اجرا میشه

برخی دیتابیس ها هم اصلا range query رو پشتیبانی نمیکنند مثل 
Riak , Couchbase , or Voldemort

کاساندرا برای داشتن امکان range-query به همراه روش Partitioning by Hash of Key یه تکنیک جالبی استفاده میکنه که compound primary key نامیده میشه

به جای انتخاب یه column به عنوان کلید، از چندین کلید به صورت ترکیبی استفاده میکنه. با کمک column اول پارتیشن مقصد انتخاب میشه. از طریق مابقی columnها مکان دقیق آبجکت در SSTables مشخص میشه.

اینجا اگر اون column اول یه مقدار فیکس باشه میشه روی column های بعدی range query زد.

حالا این روش وقتی خیلی عالی عمل میکنه که رابطه  one-to-many داشته باشیم. 

مثلا یه کاربر تعدادی پست داره. در جدول پست ها ترکیب ستون (user_id, update_timestamp) حکم همون compound primary key  رو داره که با Hash(user_id) پارتیشن مشخص میشه و با update_timestamp روی همون پارتیشن به آبجکت اصلی میرسیم. که روی update_timestamp میشه range query هم زد.

تو این سناریو پست های کاربرهای مختلف روی پارتیشن های مختلف نگهداری میشن ولی همه پست های یک کاربر همگی روی یک پارتیشن هستند.


# Skewed Workloads and Relieving Hot Spots

چجوری از پدیده Skewed یا همان  Hot Spot Partition جلوگیری کنیم؟

هش گرفتن از کلید به این مساله کمک میکنه ولی نه به طور کامل. به خصوص اگر همه نوشتن ها یا خواندن ها روی یه کلید ثابت انجام بشه تمام workload روی یک پارتیشن متمرکز میشه.

مثلا فکر کنید در یک اپلیکیشن سوشال میدیا  یه سلبریتی که تعداد فالوورهای میلیونی داره یه اکشن انجام بده و باعث بشه یه طوفانی از write یا read به سمت یه پارتیشن هجوم بیاره

معمولا دیتاسیستم ها این skewed workload رو سعی نمیکنن که بهینه هندل کنند. پس باید اپلیکیشن خودش بیاد فکری به حال این workload بکنه.


یک راه حل پیشنهادی: اپلیکیشن بیاد یک عدد رندوم به ابتدا یا انتهای کلید ضمیمه بکنه. مثلا فقط دو رقم رندوم اضافه کنه و اینطوری عملیات نوشتن روی یک کلید به صورت رندوم بین ۱۰۰ تا پارتیشن پخش میشه. ولی؟؟ موقع خوندن هم باید خودش موضوع رو هندل کنه. داده های اون ۱۰۰ تا پارتیشن رو بخونه و ترکیب کنه.

این کار رو میشه فقط روی کلیدهایی انجام داد که نرخ نوشتن روی اونها زیاده. بقیه کلیدها رو لازم نیست اینکار رو باهاش بکنیم. پس باید کلیدهای از این جنس رو هم یادمون نگه داریم.



# Partitioning and Secondary Indexes

![[Pasted image 20241007133304.png]]
در بحث پارتیشنینگ وقتی مساله Secondary Indexes مطرح میشه کار کمی پیچیده تر میشه.

قبلا که فقط ایندکس روی primary key را مدنظر داشتیم کار ساده تر بود. هر کلید فقط یک ولیوی یونیک داشت که اون کلید-ولیو فقط در یکی از پارتیشن ها قرار داشت.

اما اگر ایندکسینگ ثانوی داشته باشیم که هر کلید ولیوی منحصر به فرد نداشته باشه مپینگ کلید لزوما به یک پارتیشن مشخص نخواهد بود. 

در اینجا دوتا روش داریم:
document-based partitioning
term-based partitioning

# Partitioning Secondary Indexes by Document
در این روش که مبتنی بر داکیومنت هست،
اولا داکیومنت ها بر اساس primary key روی پارتیشن ها پخش شدند.

علاوه بر اون چون secondary indexes هم داریم هر پارتیشن فقط برای داکیومنت هایی که روی خودش هستن این ایندکس ثانویه رو هم مدیریت میکنه.


اینجا ایندکس دوم روی رنگ خودرو هست. بر اساس رنگ لیستی از آیدی داکیومنت هایی که در همون پارتیشن هستن نگه داری میشه.

هرزمان داکیومنت جدید به دیتابیس اضافه میشه (یا ازش حذف میشه)‌ دیتابیس خودش روی اون پارتیشن قسمت مربوط به ایندکس ثانوی رو هم به روز میکنه.

یه جورایی انگار هر پارتیشن یه سیستم local index داره.

اگر دیتابیس این مدل پارتیشن بندی بر اساس ایندکس ثانوی را پشتیبانی نکند میشه اونو در سطح کد پیاده سازی کرد.

موقع خوندن با ایندکس ثانویه لازمه از تمام پارتیشن ها بخونیم. به این رویکرد میگن scatter/gather. بالاخره کوئری روی ایندکس ثانویه هزینه داره حتی اگر بیاییم کوئری  روی پارتیشن های مختلف رو موازی بزنیم. با این حال کاربردیه و خیلی از دیتابیس ها ازش استفاده کردن! این دیتابیس ها پیشنهاد میکنن که طوری اسکیمای دیتاتون رو بسازید که کوئری روی ایندکس ثانویه به یک پارتیشن ختم بشه. اما خب خیلی شدنی نیست به خصوص اگر چندتا ایندکس ثانوی داشته باشیم.

![[Pasted image 20241007133236.png]]

# Partitioning Secondary Indexes by Term
به جای اینکه مثل روش قبلی ایندکس ثانوی رو به شکل  local index داشته باشیم، اینجا به روش global index عمل میکنیم.

ایندکینگ ثانویه رو هم پارتیشن بندی میکنیم روی نودها. اینجا مثلا
RED : [191,306,768]
فقط میفته روی یکی از نود ها . دیگه مثل قبلی هر نود آیدی داکیومنت های خودش رو برای کلید RED نداره!

اینجا دوباره بر اساس یه منطقی باید این ایندکسینگ ثانویه رو پخش کنیم روی پارتیشن ها.



# Rebalancing Partitions
ممکنه به هر دلیلی لازم باشه ما دیتا رو بین نودها جابه جا کنیم. که بهش میگیم Rebalancing.
مثلا چه دلیلی؟ یه ماشین خراب بشه، اندازه دیتاست انقدر زیاد باشه که لازم باشه بهش ram و disk بیشتری اختصاص بدیم و ...

چندتا نیازمندی در بحث rebalancing مطرح هست:
بعدش باید درخواست کاربران به صورت fair بین نودها پخش بشه. در هنگاه rebalancing سیستم باید درخواست کاربران رو پاسخگو باشه. تا حد ممکن سریع و کمترین داده لازم بین نودها جابه جا بشه تا نتوورک و IO زیادی مطرف نشه.

# Strategies for Rebalancing
## How not to do it: hash mod N
اگر پاتیشنینگ با هش کلید انجام شده، بهتره از روش مود گرفتن استفاده نکنیم . چون با تغییر تعداد نودها تعداد خیلی زیادی از  داده ها جابه جا خواهند شد. به جاش مثلا از روش زیر استفاده میشه:
node 1: 0 <  if hash(key) < b1
node 2: b1 <  if hash(key) < b2
...

## Fixed number of partitions
دیتا رو به تعداد پارتیشن های بیشتری (تعدادشون بیشتر از تعدا نودها باشه تقسیم کنیم طوری که چندین پارتیشن روی یک نود بیفتن. اگر یه نود اضافه بشه، تعدادی از پارتیشن ها رو از روی نودها موجود به این نود جدید انتقال میدیم. اینجا دیگه مپینگ داکیومنت به نود تغییر نمیکنه بلکه مپینگ پارتیشن به نود تغییر میکنه.
اینجا دیگه تعداد پارتیشن ها ثابت هستن. مپینگ کلید به پارتیشن هم ثابت هست.
البته این جابه جایی پارتیشن از یه نود به نود دیگه زمان زیادی طول میکشه چون اندازه دیتا بزرگه. در طول این مدت داده ها طبق assignment قبلی خونده و نوشته میشن. 
![[Pasted image 20241007133215.png]]

با این روش میشه از حداکثر توان پردازشی نود استفاده کرد. با قرار دادن تعداد خیلی زیاد پارتیشن روی اون.

در این روش معمولا تعداد پارتیشن ها موقع ساختن دیتاسیستم تنظیم میشن و بعدش تغییر نمیکنن. 
البته اساسا امکان اسپلیت کردن و مرج کردن پارتیشن ها وجود داره اما فیکس نگه داشتن تعداد پارتیشن ها از نظر عملیاتی خب خیلی ساده تره و خیلی دیتابیس ها رویکرد تعداد پارتیشن ثابت رو انتخاب میکنند و اجازه نمیدن پارتیشن ها اسپلیت یا مرج بشن.
بنابراین تعداد پارتیشن ها حداقل به تعداد نودها انتخاب میشه. ولی میشه تعدادشون رو انقدر زیاد بگیریم که بعدا کم نباشه.

البته باید بدونیم زیاد کردن تعداد پارتیشن ها سربار مدیریتی داره.
 انتخاب تعداد درست پارتیشن کار راحتی نیست. به خصوص ولی اندازه دیتا دائما در حال رشد هست. چون تعدا پارتیشن ثابت هست پس اندازه پارتیشن متناسب با رشد اندازه داده بزرگتر میشه. 

پارتیشن بزرگ هم باعث میشه rebalancing و recovery موقع fail شدن یک نود هزینه بر باشه.

## Dynamic partitioning
برای دیتابیس هایی که بر اساس رنج کلید دیتا رو پارتیشن بندی میکنن، فیکس نگه داشتن تعداد و باندری ها خیلی خوشایند نیست، چون ممکنه با رشد دیتاسایز، یک پارتیشن خیلی رشد کنه و یه پارتیشن خالی باشه. وقتی همچین اتفاقی بیفته بعدش دستی  reconfig کردن باندری ها هم کار خسته کننده ای هست.

دیتابیس های از نوع key range–partitioned مثل HBase و RethinkDB برای حل چالش اشاره شده میان از روش Dynamic partitioning استفاده میکنند.

وقتی یه پارتیشن رشد کرد و رسید به یک سایز کانفیگ شده، اون پارتیشن تقسیم میشه به دوتا پارتیشن که هر کدوم نصف دیتای پارتیشن تقسیم شده رو دارن. اگر دیلی ت شدن دیتاها باعث بشه که سایز یک پارتیشن پایین تر از یک سایز کانفیگ شده برسه اونوقت پارتیشن ها رو مرج میکنه.

هر پارتیشن داخل یک نود قرار داره و هر نود میتونه چندین پارتیشن داشته باشه داخل خودش.
وقتی یه پارتیشن اسپلیت شد، یکی شون میتونه منتقل بشه روی نود دیگری. که لود بالانس بشه.

در این شرایط HBase فایل های مربوط به اون پارتیشن رو اط طریق فایل سیستم توزیع شده خودش که HDFS نام دارد منتقل میکنه.

با این روش تعداد پارتیشن ها متناسب هستن با توتال سایز دیتا خواهد بود. سربار هم متناسب با توتال سایز دیتا خواهد بود. 

یه مساله ای که اینجا وجود داره اینه که اول کار که هیچ داده ای نداریم هنوزهیچ باندری مشخص نیست، دیتابیس کارشو با یه پارتیشن شروع میکنه. تا وقتی که سایز پارتیشن به اندازه قابل اسپلیت نرشده عملا دیتابیس یک پارتین دارد و تمام ریکوپست ها فقط به همان یک پارتیشن میرسند و بقیه نود ها سرشون حلوت هست.

مونگو و HBase برای حل این مساله میان اول کار به یک تعداد قابل کانفیگی پارتیشن اولیه ایجاد میکنند. 
روش Dynamic partitioning نه تنها برای key range–partitioned ها کاربردی هستن بلکه برای hash-partitioned ها هم به کار میان. 
در روش قبلی تعداد پارتیشن ها متناسب با توتال سایز دیتا بود. با اسپلیت و مرج کردن هم سایز هر پارتیشن در یک محدودا min تا max تغییر میکرد.

مدل سوم روشی هست که داره توسط کاساندرا استفاده میشه. اینکه یه نسبتی باشه بین تعداد پارتیشن ها و تعداد نود ها. که در دو روش قبلی هیچ نسبتی بین این دو وجود نداشت. یعنی تعداد پارتیشن ثابتی در هر نود داشته باشیم.

در این روش سایز پارتیشن ها متناسب با توتال سایز دیتا تغییر میکنه. اگر در وصرت اندازه بزرگ داده ها تعداد نود ها زیاد کنیم سایز پارتیشن ها کم میشه.  سایز پارتیشن ها هم که بالانس هستن.

وقتی یه نود جدید اضافه میشه به کلاستر، به طور تصادفی تعدادی از پارتیشن ها انتخاب میشن که نصف بشن که نصف شون بیاد روی نود جدید. این انتخاب تصادفی fair نیست. البته اگر تعداد پارتیشن های هر نود خیلی بالا باشه مثلا کاساندرا ۲۵۶ تا پارتیشن در هر نود قرار میده. اینطوری توتالی fair به حساب میاد.

باندری پارتیشن ها هم باید رندوم باشن پس از روش هش گرفتن قراره استفاده بشه. 
عملا consistent hashing داره به کار گرفته میشه.

# Operations: Automatic or Manual Rebalancing
ریبالانس کردن قراره دستی انجام بشه یا اوتوماتیک؟
اغلب یه ترکیبی از روش اتوماتیک و دستی استفاده میشه. مثلا توی Couchbase, Riak, و Voldemort اینکه چه پارتیشن هایی باید ری بالانس بشن به طور اوتوماتیک شناسایی میشن ولی انجام ری بالانسینگ به طور دستی با کامیت ادمین انجام میشه.
روش اوتوماتیک راحته چون ولی نیاز به دخالت شخصی نداره. اما از طرفی خیلی قابل اعتماد نیست. چون عملیات ری بالانسینگ خیلی پر هزینه هست اگر با دقت انجام نشه جبرانش یک عالمه زحمت داره.

روش اتوماتیک خیلی میتونه خطرناک باشه. مثلا فرض کنید یه نود سرش شلوغه و داره دیر ریکوپست ها رو جواب میده. بقیه فکر میکنن این نود مرده و اتوماتیک شروع میکنن به ری بالانس کردن. شرایط رو بدتر میکنن.

در نتیجه بهتره کاملا اتماتیک انجام نشه.


# Request Routing
تا اینجا دیتاها پایتیشن بندی شدند. الان وقتی کلاینت یمخواد یه داده رو بخونه از کجا بفهمیم درخواست کلاینت رو به کدوم نود بفرستیم؟
اگر در فرایند rebalancing پارتیشن ها بین نودها جابه جا بشنُ لازمه یکی اون بالا وایسه و تمام این اطلاعات رو داشته باشه و به سوال ما جواب بده که الان کوئری کاربر رو روی کدام ماشین اجرا کنیم؟

این مشکل نام عمومی service discovery دارد. که فقط مختص اینجا نیست. هر سرویسی که روی نتوورک قرار داده برای داشتن high availability باید چند نسخه اش روی چندتا ماشین اجرا بشن و همونجا هم service discovery  موضوع مهمی هست.

رویکردهای مختلفی برای این مساله وجود داره:
۱- کلاینت بتونه به هر نودی کانکت بشه(با یک لودبالانسر راندرابین) اگر احیانا اون نود یه پارتیشن در خودش داشت که کویری مال اون پارتیشن بود خودش همونجا ریکوئست رو جواب میده. اگر نهریکوئست رو میفرسته به نود مناسبش. ازون نود جواب رو میگیره و به کلاینت برمیگردونه.
۲- همه درخواست های کاربران اول برسه درسته یک routing tier و ایشون مشخص بکنه هر ریکوئست باید برای چه نودی ارسال بشه. میشه گفت ایشون نقش partition-aware load balancer دارن.
۳- کلاینت از تمام نود ها و موقعیت پارتیشن ها در هر نود مطلع باشه و مستقیم درخواست خودش رو به نود مناسب ارسال کنه.

در همه رویکردهای گفته شده مساله اصلی تصمیم گیری برای routing درخواست های کلاینت به نودها هست.  انتخاب بین این سه روش گفته شده

اون سه تا استراتزی بالا رو داره در قالب شکل نشون میده.
![[Pasted image 20241007133039.png]]


بسیاری از دیتاسیستم های توزیع شده میان از یک مولفه coodinator مثل ZooKeeper استفاده میکنند که متادیتای مربوط به پارتیشنینگ رو نگه بداره. هر نود خودشو به زوکیپر معرفی میکنه.

اینجا دیگه بقیه مثل routing tier و یا حتی کلاینت (تو اون روش که قراره خودش تصمیم بگیره که سراغ چه نودی بره) میتونن اطلاعات لازم رو از زوکیپر بپرسن.

اگر متادیتای دست زوکیپر تغییر کنه، زوکیپر میاد routing tier  رو آپدیت میکنه.

![[Pasted image 20241007133100.png]]

مونگو هم دراه از یه معماری مشابه استفاده میکنه. البته پیاده سازی خودش رو داره به جای زوکیپر تحت عنوان config server  و همچنین به عنوان routing tier سرویس دیمن خودشو داره تحت عنوان mongos.

کاساندرا و ریاک رویکرد متفاوتی دارند. آنها یه پروتکل gossip دارن که بین نود ها انجام میشه که نودها بین خودشون اطلاعات پارتیشن بندی رو پخش میکنن. درخواست کلاینت به دست هر نودی میتونه برسه. نود ها بین خودشون درخواست رو به مقصد درست هدایت میکنن. اینجوری نودها دارن یه پیچیدگی تحمل میکنن ولی عوضش نیاز به مولفه اضافه تری نیست

دیتابیس Couchbase ری بالانس اوتوماتیک نداره در نتیجه طراحی ساده تری داره. یه روتینگ تایر داره به اسم  moxi که ایشون روتینگ رو انجام میدن.

# Parallel Query Execution

 تا اینجا ما تمرکزمون روی کوئری های ساده بود که فقط یه رکورد رو متاثر میکردن. به علاوه کوئری های scatter/gather در مواردی که از ایندکس ثانویه استفاده میکردیم.
معمولا در دیتابیس های NoSQL سادگی کویری ها همین قدره

ولی دیتابیس های رابطه ای از نوع massively parallel processing (MPP)  که اغلب برای کارهای آنالیتیک استفاده میشن ممکنه کوٍری های پیچیده تری داشته باشن که پیچیدگی هایی مثل 
several join, filtering, grouping, and aggregation 
بخوان که
بهشون میگه: data warehouse query.


این مدل دیتابیس ها یه MPP query optimizer دارن که کويری رو میشکنه به قسمت های کوچیک تر طوری که بشه به طوری موازی روی چندین پارتیشن و نود موازی اجرا بشن. این شیوه اجرای موازی این کوئری ها موضوع فصل ۱۰ هست.

